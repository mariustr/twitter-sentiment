{"cells":[{"cell_type":"markdown","source":["### Dette er en python notebook som leses inn i Databricks.  \nVi skal gjennomføre en enkel tekstanalyse i Spark:  \n* Lese inn twittermeldinger som inneholder emnetagg #MAGA\n* Skille ut individuelle ord\n* Slette typiske 'stoppord'\n* Lese inn en sentimentordbok vi skal matche mot twitterord\n* Lage en gjennomsnittsscore for hver tweet, fra positiv til negativ\n* Lese score tilbake til originalsettet, og eksportere for visualisering av resultater (Oppgave 2)  \n\n#### Hovedformålet er å vise hvordan skyteknologi benyttes til å kverne store mengder data med SQL, pySpark eller Scala - uten betydlige forvaltningsmiljøer.  \n\n### For å kjøre koden i en celle benytt 'Play' til høyre eller benytt Ctrl+Enter.  \n  \n### Oppgaven består av å kjøre cellene, samt se på resultatet av kjøringer."],"metadata":{}},{"cell_type":"code","source":["# Vi skal benytte en blanding av python og SQL - \n# samt hente pakker fra et maskinlæringsbibliotek ml.feature for å kverne tekstdata enklere.\nfrom pyspark.sql import SQLContext, DataFrame\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.window import Window\nimport io\nimport requests\nimport urllib\nimport csv\nfrom pyspark.ml.feature import HashingTF, IDF,Tokenizer, RegexTokenizer, StopWordsRemover\nimport pandas as pd\nsqlContext = SQLContext(sc)"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":["##### I denne prosessen laster vi inn rådata, omdøper kolonner og velger å slette 'RT' fra twitterteksten.  \nDet vanligste er å benytte spark.table som kan sammenliknes med dataframes i Python/Pandas.  \n\n### * Action: benytt display() for å se på dataframe og hvilke kolonner vi mottar fra Twitter"],"metadata":{}},{"cell_type":"code","source":["#Github-datasett for analyse, inneholder 15000 tweets med emneknagg #MAGA\nurl = \"https://raw.githubusercontent.com/mariustr/twitter-sentiment/master/twitterdata.csv\"\nurllib.urlretrieve(url, 'twitterdata.csv')\n#Les inn temp-fil som Spark-tabell:\ndf = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true', delimiter = ';').load('file:/databricks/driver/twitterdata.csv')\n#display(df)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["###### Før vi henter ut enkeltord i twittermedldingene, vil vi fjerne tegnsetting og gjøre om all tekst til små bokstaver.  \nDeretter separerer vi ut enkeltord fra twittermeldingen. Tokenizer-pakken hjelper med prosessen. Her velger vi også å lage en ny tabell med færre kolonner før vi fortsetter analysen.  \nFor mer avansert tekstprosessering kan man også benytte Tokenizer.Regex-funksjoner.  \n\n### * Action: For Tokenizer-funksjonen, fyll inn inputkolonne for twittermeldinger, og ny outputkolonne for resultatet. Outputkolonnen skal hete \"words\"."],"metadata":{}},{"cell_type":"code","source":["#Gjør om til små bokstaver og flern tegnsetting\ndf = df.withColumn('text', lower(df.text))\ndf = df.withColumn('text', regexp_replace('text',r'[^a-z0-9\\s]',''))\n# Benytt Tokonizer i pyspark.ml-pakke for å skille ut enkeltord\ntokenizer = Tokenizer(inputCol=\"text\",outputCol=\"words\")\ntweetData = tokenizer.transform(df)\n#Begrenser datasettet til 3 kolonner for videreanalyse. Originaltabell 'df' vil bestå.\ntweetData = tweetData.select('id', 'text','words')\n#display(df)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\nfeaturizedData = hashingTF.transform(tweetData)\nfeaturizedData.show(5)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["##### Trekk ut stoppord, de mest brukte nøytrale ordene fra en engelsk ordliste (default).  \n### Action: Velg inputkolonne hvor du skal slette stoppord. Velg nytt navn på outputkolonne."],"metadata":{}},{"cell_type":"code","source":["remover = StopWordsRemover(inputCol=\"words\", outputCol = \"filtered\")\ntweetDataStopRemoved = remover.transform(tweetData)\n#display(df2)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["##### Last inn sentimentdata fra Github. Vi skal benytte et datasett med enkeltord, i neste iterasjon kunne vi benyttet et annet datasett som har score per kombinasjon av to ord."],"metadata":{}},{"cell_type":"code","source":["#Les fra Github\nurl2 = \"https://raw.githubusercontent.com/mariustr/twitter-sentiment/master/S140-AFFLEX-NEGLEX-unigrams.txt\"\nurllib.urlretrieve(url2, 'sentimentdata.txt')\n#Les inn som Sparktabell\nsentimentoppslag = sqlContext.read.format('com.databricks.spark.csv').options(header='false', inferschema='true', delimiter = '\\t').load('file:/databricks/driver/sentimentdata.txt')\n#Skriv om kolonnenavn\nsentimentoppslag = sentimentoppslag.withColumnRenamed(\"_c0\", \"sentimentword\")\nsentimentoppslag = sentimentoppslag.withColumnRenamed(\"_c1\", \"sentimentscore\")\n#Behold bare de to første kolonnene\nsentimentoppslag = sentimentoppslag.select(\"sentimentword\", \"sentimentscore\")\n#Skift datatype fra tekst til desimal for scoringskolonnen\nsentimentoppslag = sentimentoppslag.withColumn(\"sentimentscore\", sentimentoppslag.sentimentscore.cast('float'))\n#display(sentimentoppslag)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["tweetsentimentframe = tweetDataStopRemoved.select('id',explode('filtered').alias('sentimentword')).join(sentimentoppslag, 'sentimentword')\n#display(tweetsentimentframe)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["sentiment_prediction = tweetsentimentframe.groupBy('id').agg(avg('sentimentscore').alias('snittscore'))\ndisplay(sentiment_prediction)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["tweetDataSentimentScore = df.join(sentiment_prediction, \"id\")\n#tweetDataSentimentScore.sort(\"id\").desc().collect()\ndisplay(tweetDataSentimentScore)"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["sentiment_prediction.SkrivTilS3('/dbfs/FileStore/tables/scores.csv', sep=',', header=True, index=False)\nsentiment_prediction.SkrivTilRedshiftDB('/dbfs/FileStore/tables/scores.csv', sep=',', header=True, index=False)"],"metadata":{},"outputs":[],"execution_count":15}],"metadata":{"name":"Text Mining Exercise","notebookId":555028469781502},"nbformat":4,"nbformat_minor":0}
